{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d9f87f1",
   "metadata": {},
   "source": [
    "#### There are several types of analysis in data analysis, including:\n",
    "\n",
    "#### Descriptive analysis: This involves summarizing and describing the characteristics of a dataset, such as mean, median, mode, standard deviation, and frequency distribution.\n",
    "\n",
    "#### Inferential analysis: This involves using statistical techniques to draw conclusions about a population based on a sample of data.\n",
    "\n",
    "#### Predictive analysis: This involves using data to make predictions about future events or trends, such as forecasting sales for the next quarter.\n",
    "\n",
    "#### Prescriptive analysis: This involves using data to identify the best course of action or decision to take, such as optimizing a supply chain to reduce costs.\n",
    "\n",
    "#### Diagnostic analysis: This involves identifying the cause of a particular outcome or problem, such as determining why sales have decreased in a particular region.\n",
    "\n",
    "#### Exploratory analysis: This involves investigating a dataset to discover patterns, relationships, and insights that may not be immediately apparent, such as using clustering techniques to identify customer segments.\n",
    "\n",
    "#### Textual analysis: This involves analyzing unstructured data such as text, using techniques such as natural language processing to extract meaning and insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5585f8f",
   "metadata": {},
   "source": [
    "# Descriptive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c607d6d6",
   "metadata": {},
   "source": [
    "### Descriptive analysis is a type of data analysis that involves summarizing and describing the characteristics of a dataset. This can include measures of central tendency (e.g., mean, median, mode) and measures of variability (e.g., standard deviation, range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b5e3942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 14.127291739894563\n",
      "Median: 13.37\n",
      "Mode: 0    12.34\n",
      "Name: radius_mean, dtype: float64\n",
      "Standard Deviation: 3.524048826212078\n",
      "Range: 21.128999999999998\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe\n",
    "df = pd.read_csv('example_dataset.csv')\n",
    "\n",
    "# Print the first five rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Calculate the mean, median, and mode of a numerical variable in the dataset\n",
    "print(\"Mean:\", df['numerical_variable'].mean())\n",
    "print(\"Median:\", df['numerical_variable'].median())\n",
    "print(\"Mode:\", df['numerical_variable'].mode())\n",
    "\n",
    "# Calculate the standard deviation and range of the numerical variable\n",
    "print(\"Standard Deviation:\", df['numerical_variable'].std())\n",
    "print(\"Range:\", df['numerical_variable'].max() - df['numerical_variable'].min())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26aac56",
   "metadata": {},
   "source": [
    "#### In this example code, we first load a dataset into a pandas dataframe using the read_csv function. We then use the head function to print the first five rows of the dataset to get a quick overview of the data.\n",
    "\n",
    "#### Next, we calculate several measures of central tendency and variability for a numerical variable in the dataset (numerical_variable). We use the mean, median, and mode functions to calculate the mean, median, and mode of the variable, respectively. We then use the std function to calculate the standard deviation of the variable, and the max and min functions to calculate the range of the variable. These measures can provide insights into the distribution and characteristics of the variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6379788",
   "metadata": {},
   "source": [
    "#  Inferential analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7fdfdb",
   "metadata": {},
   "source": [
    "### Inferential analysis is a type of statistical analysis that involves drawing conclusions about a population based on a sample of data. Inferential analysis uses statistical techniques to make inferences about a population based on the characteristics of a sample, such as confidence intervals and hypothesis testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3208afcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval: [13.837729548191374, 14.416853931597752]\n",
      "t-statistic:  nan\n",
      "p-value:  nan\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Load the dataset into a pandas dataframe\n",
    "df = pd.read_csv('example_dataset.csv')\n",
    "\n",
    "# Calculate the mean of a numerical variable in the dataset\n",
    "sample_mean = df['numerical_variable'].mean()\n",
    "\n",
    "# Calculate the standard error of the mean\n",
    "standard_error = df['numerical_variable'].std() / np.sqrt(len(df))\n",
    "\n",
    "# Calculate a 95% confidence interval for the population mean\n",
    "lower_bound = sample_mean - 1.96 * standard_error\n",
    "upper_bound = sample_mean + 1.96 * standard_error\n",
    "print(\"95% Confidence Interval: [{}, {}]\".format(lower_bound, upper_bound))\n",
    "\n",
    "# Test whether the mean of a numerical variable is significantly different between two groups\n",
    "group1 = df[df['group'] == 'A']['numerical_variable']\n",
    "group2 = df[df['group'] == 'B']['numerical_variable']\n",
    "t_stat, p_val = ttest_ind(group1, group2, equal_var=False)\n",
    "print(\"t-statistic: \", t_stat)\n",
    "print(\"p-value: \", p_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31180491",
   "metadata": {},
   "source": [
    "#### In this example code, we first load a dataset into a pandas dataframe using the read_csv function. We then calculate the sample mean and standard error of a numerical variable (numerical_variable) in the dataset.\n",
    "\n",
    "#### Next, we use these values to calculate a 95% confidence interval for the population mean of the variable. This interval provides an estimate of the range of values within which we can be reasonably confident that the true population mean lies.\n",
    "\n",
    "#### Finally, we test whether the mean of the numerical variable is significantly different between two groups (group1 and group2) using a t-test. The ttest_ind function from the scipy.stats library calculates the t-statistic and p-value for this test. The p-value indicates the probability of observing a difference in means as large as or larger than the one observed, assuming that the null hypothesis (that the means are equal) is true. If the p-value is below a predetermined significance level (e.g., 0.05), we reject the null hypothesis and conclude that the means are significantly different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d5ff0e",
   "metadata": {},
   "source": [
    "# Predictive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb9b59f",
   "metadata": {},
   "source": [
    "### Predictive analysis is a type of data analysis that involves using data to make predictions about future events or trends. Predictive analysis uses techniques such as regression analysis, time series analysis, and machine learning to develop models that can forecast future outcomes based on historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9daf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the dataset into a pandas dataframe\n",
    "df = pd.read_csv('df.csv')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['feature1', 'feature2']], df['target_variable'], test_size=0.2)\n",
    "\n",
    "# Train a linear regression model on the training set\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained model to make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model using mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error: \", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd6dc82",
   "metadata": {},
   "source": [
    "#### In this example code, we first load a dataset into a pandas dataframe using the read_csv function. We then split the dataset into training and testing sets using the train_test_split function from scikit-learn.\n",
    "\n",
    "#### Next, we train a linear regression model on the training set using the LinearRegression class from scikit-learn. We then use the trained model to make predictions on the testing set using the predict method.\n",
    "\n",
    "#### Finally, we evaluate the performance of the model on the testing set using mean squared error (MSE). The mean_squared_error function from scikit-learn calculates the MSE between the predicted and actual values of the target variable. The MSE is a measure of the average squared difference between the predicted and actual values, and a lower MSE indicates better predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d31b75",
   "metadata": {},
   "source": [
    "# Prescriptive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c4f119",
   "metadata": {},
   "source": [
    "### Prescriptive analysis is a type of data analysis that involves using data and models to make recommendations for actions that will optimize a particular outcome. Prescriptive analysis builds on predictive analysis by incorporating constraints and objectives to identify the best course of action given the available options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b425c0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pulp import *\n",
    "\n",
    "# Create a linear optimization problem\n",
    "prob = LpProblem(\"Example Problem\", LpMaximize)\n",
    "\n",
    "# Define the decision variables\n",
    "x = LpVariable(\"x\", 0, None)\n",
    "y = LpVariable(\"y\", 0, None)\n",
    "\n",
    "# Define the objective function to be maximized\n",
    "prob += 2*x + 3*y\n",
    "\n",
    "# Add constraints\n",
    "prob += x + y <= 4\n",
    "prob += x <= 2\n",
    "prob += y <= 3\n",
    "\n",
    "# Solve the problem\n",
    "prob.solve()\n",
    "\n",
    "# Print the optimal values of the decision variables and the objective function\n",
    "print(\"x: \", x.value())\n",
    "print(\"y: \", y.value())\n",
    "print(\"Optimal Objective Function Value: \", value(prob.objective))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b65c510",
   "metadata": {},
   "source": [
    "#### In this example code, we first create a linear optimization problem using the LpProblem class from the PuLP library. We then define two decision variables, x and y, and an objective function to be maximized (in this case, 2*x + 3*y).\n",
    "\n",
    "#### We then add three constraints to the problem using the += operator. These constraints limit the values of x and y and ensure that their sum is no greater than 4.\n",
    "\n",
    "#### Finally, we solve the optimization problem using the solve method, and print the optimal values of the decision variables and the objective function using the value method. In this example, the optimal values are x=2 and y=2, and the optimal objective function value is 10. This indicates that the best course of action is to set x=2 and y=2 in order to maximize the objective function, subject to the given constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b89902",
   "metadata": {},
   "source": [
    "#  Diagnostic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94796c88",
   "metadata": {},
   "source": [
    "### Diagnostic analysis is a type of data analysis that involves identifying the cause of a problem or issue by examining data and other information. Diagnostic analysis often involves visualizations and statistical techniques to identify patterns and trends in the data that can explain the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784df70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset into a pandas dataframe\n",
    "df = pd.read_csv('example_dataset.csv')\n",
    "\n",
    "# Compute summary statistics of the dataset\n",
    "summary = df.describe()\n",
    "\n",
    "# Create a pairplot of the dataset\n",
    "sns.pairplot(df)\n",
    "\n",
    "# Create a heatmap of the correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67674bbd",
   "metadata": {},
   "source": [
    "#### In this example code, we first load a dataset into a pandas dataframe using the read_csv function. We then compute summary statistics of the dataset using the describe method, which calculates the count, mean, standard deviation, minimum, and maximum values of each column in the dataframe.\n",
    "\n",
    "#### We then create a pairplot of the dataset using the pairplot function from the seaborn library. A pairplot is a scatterplot matrix that shows the relationship between every pair of variables in the dataset, and can be used to identify patterns and trends in the data.\n",
    "\n",
    "#### Finally, we create a heatmap of the correlation matrix using the heatmap function from seaborn. The correlation matrix shows the correlation coefficient between every pair of variables in the dataset, and can be used to identify which variables are most strongly related to each other. In this example, we use a colormap called 'coolwarm' to indicate the strength and direction of the correlation coefficient, and we annotate each cell with the actual value of the correlation coefficient. This can help identify relationships that might be causing the issue or problem under investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a387b51",
   "metadata": {},
   "source": [
    "# Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84ecbbd",
   "metadata": {},
   "source": [
    "### Exploratory analysis is a type of data analysis that involves exploring and summarizing a dataset to gain insights and generate hypotheses about the relationships and patterns in the data. Exploratory analysis often involves data visualization and summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e81867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset into a pandas dataframe\n",
    "df = pd.read_csv('example_dataset.csv')\n",
    "\n",
    "# Print the first five rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Compute summary statistics of the dataset\n",
    "summary = df.describe()\n",
    "print(summary)\n",
    "\n",
    "# Create a histogram of one variable in the dataset\n",
    "plt.hist(df['variable'], bins=20)\n",
    "plt.xlabel('Variable Name')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Create a scatterplot of two variables in the dataset\n",
    "plt.scatter(df['variable1'], df['variable2'])\n",
    "plt.xlabel('Variable 1')\n",
    "plt.ylabel('Variable 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8916ebdd",
   "metadata": {},
   "source": [
    "#### In this example code, we first load a dataset into a pandas dataframe using the read_csv function. We then print the first five rows of the dataset using the head method to get an idea of what the data looks like.\n",
    "\n",
    "#### We then compute summary statistics of the dataset using the describe method, which calculates the count, mean, standard deviation, minimum, and maximum values of each column in the dataframe.\n",
    "\n",
    "#### We then create a histogram of one variable in the dataset using the hist function from matplotlib. The histogram shows the distribution of values of that variable in the dataset, and can be used to identify any patterns or outliers.\n",
    "\n",
    "#### Finally, we create a scatterplot of two variables in the dataset using the scatter function from matplotlib. The scatterplot shows the relationship between the two variables, and can be used to identify any trends or patterns in the data. In this example, we plot variable1 on the x-axis and variable2 on the y-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9f43b1",
   "metadata": {},
   "source": [
    "# Textual analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612d3b65",
   "metadata": {},
   "source": [
    "### Textual analysis is a type of data analysis that involves analyzing written or spoken language to extract meaningful insights and patterns. This type of analysis is often used in fields such as linguistics, marketing, and social sciences to analyze written text or transcripts of conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0ed6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# Load the movie reviews dataset\n",
    "nltk.download('movie_reviews')\n",
    "reviews = [(list(movie_reviews.words(fileid)), category)\n",
    "           for category in movie_reviews.categories()\n",
    "           for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Print the first review in the dataset\n",
    "print(reviews[0])\n",
    "\n",
    "# Convert the reviews to lowercase and remove punctuation\n",
    "reviews_cleaned = []\n",
    "for words, category in reviews:\n",
    "    words_cleaned = [word.lower() for word in words if word.isalpha()]\n",
    "    reviews_cleaned.append((words_cleaned, category))\n",
    "\n",
    "# Compute the frequency distribution of words in positive reviews\n",
    "positive_reviews = [words for words, category in reviews_cleaned if category == 'pos']\n",
    "positive_words = [word for review in positive_reviews for word in review]\n",
    "positive_word_freq = nltk.FreqDist(positive_words)\n",
    "print(positive_word_freq.most_common(10))\n",
    "\n",
    "# Compute the frequency distribution of words in negative reviews\n",
    "negative_reviews = [words for words, category in reviews_cleaned if category == 'neg']\n",
    "negative_words = [word for review in negative_reviews for word in review]\n",
    "negative_word_freq = nltk.FreqDist(negative_words)\n",
    "print(negative_word_freq.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dfd808",
   "metadata": {},
   "source": [
    "#### In this example code, we first load the movie reviews dataset from the NLTK corpus using the movie_reviews module. The dataset consists of 1000 positive and 1000 negative reviews of movies.\n",
    "\n",
    "#### We then print the first review in the dataset to get an idea of what the data looks like.\n",
    "\n",
    "#### Next, we clean the reviews by converting them to lowercase and removing punctuation using a for loop. We store the cleaned reviews in a new list called reviews_cleaned.\n",
    "\n",
    "#### Finally, we compute the frequency distribution of words in positive and negative reviews using the FreqDist function from NLTK. We first extract all the words from the positive and negative reviews into separate lists called positive_words and negative_words. We then use the FreqDist function to count the frequency of each word in each list, and print the 10 most common words in each list. This can help identify which words are most frequently associated with positive and negative reviews, and can be used to generate insights about the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
